{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  ...  Age  Outcome\n",
       "0              6      148  ...   50        1\n",
       "1              1       85  ...   31        0\n",
       "2              8      183  ...   32        1\n",
       "3              1       89  ...   21        0\n",
       "4              0      137  ...   33        1\n",
       "..           ...      ...  ...  ...      ...\n",
       "763           10      101  ...   63        0\n",
       "764            2      122  ...   27        0\n",
       "765            5      121  ...   30        0\n",
       "766            1      126  ...   47        1\n",
       "767            1       93  ...   23        0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ FROM CSV AND CREATE A DATA FRAME\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../datasets/diabetes.csv')\n",
    "df\n",
    "\n",
    "# df_male = df.loc[df['gender'] == 'Male']    # GET ONLY THE MALE DATA(REASON: APATOTO FEMALE DATA THAK)\n",
    "\n",
    "# df = df.drop(\"gender\",axis=\"columns\")     # REMOVE GENDER COLUMN\n",
    "\n",
    "# df['gender'].replace('Male', 1, inplace=True)\n",
    "# df['gender'].replace('Female', 0, inplace=True)\n",
    "# df = df.loc[df['gender'] != 'Other']\n",
    "\n",
    "# df = df.drop(\"smoking_history\",axis=\"columns\")     # REMOVE SMOKING HISTORY COLUMN(REASON: DONT KNOW HOW SMOKING AFFECTS DIABETES)\n",
    "# df_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']\n",
    "# for i in col:\n",
    "#   df[i].replace(0,df[i].mean(),inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT DF_MALE INTO X & Y\n",
    "# X = INDEPENDENT VARIABLES\n",
    "# Y = EPENDENT VARIABLES\n",
    "y = df['Outcome'].copy()\n",
    "X = df.drop('Outcome', axis=\"columns\")\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLE ADTA IMBALANCE - INCREASE/DECREASE ROWS WITH CLASS VALUES(0/1) IF ONE IS LOWER IN NUMBER\n",
    "# FOR BETTER TRAINING\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X_balanced, y_balanced = SMOTE().fit_resample(X, y)\n",
    "# y_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Scale X with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 32)                288       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,929\n",
      "Trainable params: 2,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Dense(32, input_shape=(8,), activation='relu'))\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# model.add(keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=8, activation= 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary(\n",
    "    expand_nested=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 1ms/step - loss: 0.6721 - accuracy: 0.5879\n",
      "Epoch 2/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5492 - accuracy: 0.7508\n",
      "Epoch 3/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7720\n",
      "Epoch 4/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7704\n",
      "Epoch 5/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7866\n",
      "Epoch 6/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.7932\n",
      "Epoch 7/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4340 - accuracy: 0.7932\n",
      "Epoch 8/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.7850\n",
      "Epoch 9/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.7997\n",
      "Epoch 10/150\n",
      "62/62 [==============================] - 0s 994us/step - loss: 0.4160 - accuracy: 0.7964\n",
      "Epoch 11/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.7964\n",
      "Epoch 12/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4061 - accuracy: 0.7980\n",
      "Epoch 13/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4009 - accuracy: 0.8078\n",
      "Epoch 14/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.8062\n",
      "Epoch 15/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.8062\n",
      "Epoch 16/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8257\n",
      "Epoch 17/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8111\n",
      "Epoch 18/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3783 - accuracy: 0.8225\n",
      "Epoch 19/150\n",
      "62/62 [==============================] - 0s 983us/step - loss: 0.3708 - accuracy: 0.8355\n",
      "Epoch 20/150\n",
      "62/62 [==============================] - 0s 1000us/step - loss: 0.3671 - accuracy: 0.8257\n",
      "Epoch 21/150\n",
      "62/62 [==============================] - 0s 994us/step - loss: 0.3653 - accuracy: 0.8355\n",
      "Epoch 22/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8225\n",
      "Epoch 23/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3558 - accuracy: 0.8371\n",
      "Epoch 24/150\n",
      "62/62 [==============================] - 0s 989us/step - loss: 0.3523 - accuracy: 0.8420\n",
      "Epoch 25/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3451 - accuracy: 0.8453\n",
      "Epoch 26/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8436\n",
      "Epoch 27/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8404\n",
      "Epoch 28/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8469\n",
      "Epoch 29/150\n",
      "62/62 [==============================] - 0s 999us/step - loss: 0.3317 - accuracy: 0.8518\n",
      "Epoch 30/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3231 - accuracy: 0.8518\n",
      "Epoch 31/150\n",
      "62/62 [==============================] - 0s 987us/step - loss: 0.3194 - accuracy: 0.8453\n",
      "Epoch 32/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8664\n",
      "Epoch 33/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.8550\n",
      "Epoch 34/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8550\n",
      "Epoch 35/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8664\n",
      "Epoch 36/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8599\n",
      "Epoch 37/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2940 - accuracy: 0.8664\n",
      "Epoch 38/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8697\n",
      "Epoch 39/150\n",
      "62/62 [==============================] - 0s 1000us/step - loss: 0.2872 - accuracy: 0.8779\n",
      "Epoch 40/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2777 - accuracy: 0.8811\n",
      "Epoch 41/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.8746\n",
      "Epoch 42/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8827\n",
      "Epoch 43/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.8893\n",
      "Epoch 44/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2624 - accuracy: 0.8909\n",
      "Epoch 45/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8941\n",
      "Epoch 46/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.8974\n",
      "Epoch 47/150\n",
      "62/62 [==============================] - 0s 993us/step - loss: 0.2502 - accuracy: 0.8990\n",
      "Epoch 48/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.9055\n",
      "Epoch 49/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2361 - accuracy: 0.9104\n",
      "Epoch 50/150\n",
      "62/62 [==============================] - 0s 987us/step - loss: 0.2406 - accuracy: 0.9039\n",
      "Epoch 51/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9007\n",
      "Epoch 52/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9104\n",
      "Epoch 53/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.9121\n",
      "Epoch 54/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9137\n",
      "Epoch 55/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9104\n",
      "Epoch 56/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9267\n",
      "Epoch 57/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9218\n",
      "Epoch 58/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9251\n",
      "Epoch 59/150\n",
      "62/62 [==============================] - 0s 981us/step - loss: 0.2008 - accuracy: 0.9218\n",
      "Epoch 60/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9316\n",
      "Epoch 61/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9283\n",
      "Epoch 62/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9381\n",
      "Epoch 63/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9365\n",
      "Epoch 64/150\n",
      "62/62 [==============================] - 0s 991us/step - loss: 0.1766 - accuracy: 0.9381\n",
      "Epoch 65/150\n",
      "62/62 [==============================] - 0s 988us/step - loss: 0.1719 - accuracy: 0.9332\n",
      "Epoch 66/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9414\n",
      "Epoch 67/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9463\n",
      "Epoch 68/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9414\n",
      "Epoch 69/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9495\n",
      "Epoch 70/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9495\n",
      "Epoch 71/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9414\n",
      "Epoch 72/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9544\n",
      "Epoch 73/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9560\n",
      "Epoch 74/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9577\n",
      "Epoch 75/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9560\n",
      "Epoch 76/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9577\n",
      "Epoch 77/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9658\n",
      "Epoch 78/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9593\n",
      "Epoch 79/150\n",
      "62/62 [==============================] - 0s 997us/step - loss: 0.1188 - accuracy: 0.9674\n",
      "Epoch 80/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9544\n",
      "Epoch 81/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9788\n",
      "Epoch 82/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9658\n",
      "Epoch 83/150\n",
      "62/62 [==============================] - 0s 986us/step - loss: 0.1035 - accuracy: 0.9756\n",
      "Epoch 84/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9772\n",
      "Epoch 85/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9788\n",
      "Epoch 86/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9837\n",
      "Epoch 87/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9756\n",
      "Epoch 88/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9837\n",
      "Epoch 89/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9837\n",
      "Epoch 90/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9788\n",
      "Epoch 91/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9772\n",
      "Epoch 92/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9805\n",
      "Epoch 93/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9870\n",
      "Epoch 94/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9853\n",
      "Epoch 95/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9821\n",
      "Epoch 96/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9772\n",
      "Epoch 97/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9853\n",
      "Epoch 98/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9902\n",
      "Epoch 99/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9772\n",
      "Epoch 100/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.9853\n",
      "Epoch 101/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9902\n",
      "Epoch 102/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9886\n",
      "Epoch 103/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9853\n",
      "Epoch 104/150\n",
      "62/62 [==============================] - 0s 976us/step - loss: 0.0576 - accuracy: 0.9902\n",
      "Epoch 105/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9886\n",
      "Epoch 106/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9902\n",
      "Epoch 107/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0493 - accuracy: 0.9935\n",
      "Epoch 108/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9886\n",
      "Epoch 109/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9902\n",
      "Epoch 110/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9935\n",
      "Epoch 111/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0454 - accuracy: 0.9870\n",
      "Epoch 112/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9870\n",
      "Epoch 113/150\n",
      "62/62 [==============================] - 0s 985us/step - loss: 0.0416 - accuracy: 0.9935\n",
      "Epoch 114/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0379 - accuracy: 0.9935\n",
      "Epoch 115/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9935\n",
      "Epoch 116/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9935\n",
      "Epoch 117/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9935\n",
      "Epoch 118/150\n",
      "62/62 [==============================] - 0s 998us/step - loss: 0.0380 - accuracy: 0.9902\n",
      "Epoch 119/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0369 - accuracy: 0.9919\n",
      "Epoch 120/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0347 - accuracy: 0.9935\n",
      "Epoch 121/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 0.9951\n",
      "Epoch 122/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.9951\n",
      "Epoch 123/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9951\n",
      "Epoch 124/150\n",
      "62/62 [==============================] - 0s 993us/step - loss: 0.0262 - accuracy: 0.9951\n",
      "Epoch 125/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9935\n",
      "Epoch 126/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0266 - accuracy: 0.9935\n",
      "Epoch 127/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9886\n",
      "Epoch 128/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9723\n",
      "Epoch 129/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9821\n",
      "Epoch 130/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9902\n",
      "Epoch 131/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9935\n",
      "Epoch 132/150\n",
      "62/62 [==============================] - 0s 993us/step - loss: 0.0234 - accuracy: 0.9951\n",
      "Epoch 133/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9951\n",
      "Epoch 134/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9951\n",
      "Epoch 135/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9935\n",
      "Epoch 136/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 137/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9935\n",
      "Epoch 138/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9951\n",
      "Epoch 139/150\n",
      "62/62 [==============================] - 0s 993us/step - loss: 0.0165 - accuracy: 0.9951\n",
      "Epoch 140/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9951\n",
      "Epoch 141/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9951\n",
      "Epoch 142/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 0.9967\n",
      "Epoch 143/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9951\n",
      "Epoch 144/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0154 - accuracy: 0.9951\n",
      "Epoch 145/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0380 - accuracy: 0.9902\n",
      "Epoch 146/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9870\n",
      "Epoch 147/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9853\n",
      "Epoch 148/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9935\n",
      "Epoch 149/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9935\n",
      "Epoch 150/150\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "# earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=10,\n",
    "#     # callbacks=[earlystop_loss],\n",
    "# )\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 1.8836 - accuracy: 0.7143\n",
      "Accuracy: 71.43\n",
      "1.8836065530776978\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs run: 150\n",
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(acc) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Epochs run:\", len(history.history[\"loss\"]))\n",
    "\n",
    "print(history.history.keys())\n",
    "acc = history.history[\"accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Train and validation accuracy\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylim((0, 1))\n",
    "plt.plot(epochs, acc, label=\"Training accurarcy\")\n",
    "plt.plot(epochs, val_acc, label=\"Validation accurarcy\")\n",
    "plt.title(\"Training and Validation accurarcy\")\n",
    "plt.legend()\n",
    "\n",
    "# Train and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(epochs, loss, label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER TUNING\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# # defining parameter range\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf']} \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# # fitting the model for grid search\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # print best parameter after tuning\n",
    "# print(grid.best_params_)\n",
    "  \n",
    "# # print how our model looks after hyper-parameter tuning\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# PREDICT RESULTS\n",
    "\n",
    "# make probability predictions with the model\n",
    "y_pred = model.predict(X_test)\n",
    "# round predictions \n",
    "y_pred_rounded = [round(x[0]) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONFUSION MATRIX\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "cm = confusion_matrix(y_test, y_pred_rounded)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1618\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3941\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3946\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3947\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3948\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3949\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 3932\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3934\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3938\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:960\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_indices:\n\u001b[1;32m--> 960\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    962\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:284\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 284\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# MANUAL CHECKING BY PUTTING ROW VALUES\u001b[39;00m\n\u001b[0;32m      3\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m800\u001b[39m\n\u001b[1;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m res \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction => \u001b[39m\u001b[38;5;124m'\u001b[39m, res)\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1651\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32mc:\\Users\\Gourab Paul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1621\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n\u001b[1;32m-> 1621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# MANUAL CHECKING BY PUTTING ROW VALUES\n",
    "\n",
    "row = 800\n",
    "y_pred = model.predict(X_test.iloc[[row]])\n",
    "res = y_pred[0]\n",
    "print('prediction => ', res)\n",
    "print('real => ', y_test.iloc[[row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6b71e845-940e-40e7-9626-17c78c505ffe/assets\n"
     ]
    }
   ],
   "source": [
    "# SAVING THE MODEL USING PICKLE PACKAGE\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"diabetes-model-ann.pkl\"  \n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOAD AND USE THE SAVED MODEL USING PICKLE PACKAGE\n",
    "# with open(model_pkl_file, 'rb') as file:  \n",
    "#     model = pickle.load(file)\n",
    "\n",
    "# # evaluate model \n",
    "# y_predict = model.predict(X_test)\n",
    "\n",
    "# # check results\n",
    "# pred = model.evaluate(X_test, y_test)\n",
    "# print(f\"Accuracy : {pred * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
