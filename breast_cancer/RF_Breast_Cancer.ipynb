{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./bc_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "clms = df.columns\n",
    "print(clms)\n",
    "### remove id column and unnamed column\n",
    "df = df.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "\n",
    "### checking null values\n",
    "df.isnull().sum()\n",
    "\n",
    "### checking info \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### rename target column\n",
    "df = df.rename(columns={'diagnosis': 'target'})\n",
    "### target column replace with 0 and 1\n",
    "df['target'] = df['target'].replace({'B': 0, 'M': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### Define a color palette (you can customize this list of colors)\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "\n",
    "### Create the plot with different colors for each bar\n",
    "ax = sns.countplot(x='target', data=df, palette=colors)\n",
    "\n",
    "### Add count on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), \n",
    "                textcoords='offset points')\n",
    "\n",
    "### Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### check correlation with target\n",
    "corr_metrix = df.corr()\n",
    "corr_metrix['target']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### correlation matrix in heatmap\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(corr_metrix, annot=True,cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### divide dependent and independent variables\n",
    "x = df.drop([\"target\"], axis = 1)\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE(sampling_strategy=\"minority\", random_state=42, k_neighbors=10, n_jobs=-1)\n",
    "x, y = oversample.fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### let's divide train and test samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)\n",
    "l0, l1 = 0, 0\n",
    "for i in y:\n",
    "    if i==0:\n",
    "        l0 += 1\n",
    "    else:\n",
    "        l1 += 1\n",
    "print(l0,l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "### Random Forest classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, criterion='gini', max_features='sqrt')\n",
    "rf.fit(X_train,y_train)\n",
    "rf_train_acc = rf.score(X_train,y_train)*100\n",
    "rf_test_acc = rf.score(X_test,y_test)*100\n",
    "print('Training Acc-> ',rf_train_acc)\n",
    "print('Testing Acc -> ', rf_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Score = {}\n",
    "n_estimators = [10,20,30,50,75]\n",
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "max_features = ['sqrt', 'log2']\n",
    "for n in n_estimators:\n",
    "    for c in criterion:\n",
    "        for mf in max_features:\n",
    "            model = RandomForestClassifier(n_estimators=n,criterion=c,max_features=mf)\n",
    "            model.fit(X_train,y_train)\n",
    "            acc = model.score(X_test,y_test)*100\n",
    "            Score[n,c,mf] = acc\n",
    "            \n",
    "max(Score.values()) # 85.15625\n",
    "tunedVal = max(Score,key=Score.get)\n",
    "tunedVal,max(Score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper parameter tuning of random forest\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# grid_param = {\n",
    "#     'n_estimators' : [10,20,30,50,75],\n",
    "#     'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "#     'max_depth' : [7, 10, 13, 15],\n",
    "#     'class_weight': ['balanced', \n",
    "#                     #  'balanced_subsample'\n",
    "#                      ],\n",
    "#     'min_samples_leaf' : [1, 3, 5, 7],\n",
    "#     'min_samples_split' : [2, 3, 5, 7],\n",
    "#     'max_features' : ['sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# grid_search_rf = GridSearchCV(rf, grid_param, cv = 5, n_jobs = -1, verbose = 3)\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# # best parameters and best score\n",
    "# print(grid_search_rf.best_params_)\n",
    "# print(grid_search_rf.best_score_)\n",
    "# print(grid_search_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest classification after imabalnce and grid search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(class_weight='balanced',n_estimators=50,criterion='gini',max_depth=10,max_features='sqrt',\n",
    "                            # min_samples_leaf=3,# min_samples_split=2,\n",
    "                        )\n",
    "rf.fit(X_train,y_train)\n",
    "rf_train_acc = rf.score(X_train,y_train)*100\n",
    "rf_test_acc = rf.score(X_test,y_test)*100\n",
    "print('Training Acc-> ',rf_train_acc)\n",
    "print('Testing Acc -> ', rf_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "print(y_pred)\n",
    "from sklearn.metrics import confusion_matrix, classification_report,mean_absolute_error,mean_squared_error\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "f1_score = 2*(0.97*1/(0.97+1))\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print('mae => ', mae)\n",
    "print('mse => ', mse)\n",
    "print('rmse => ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL USING PICKLE PACKAGE\n",
    "import pickle\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"./bc-rf.pkl\"\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "# LOAD AND USE THE SAVED MODEL USING PICKLE PACKAGE\n",
    "with open(model_pkl_file, 'rb') as file:  \n",
    "    loaded_rf = pickle.load(file)\n",
    "    y_pred = loaded_rf.predict(X_test)\n",
    "    # check results\n",
    "    pred = loaded_rf.score(X_test, y_test)\n",
    "    print(f\"Accuracy : {pred * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
