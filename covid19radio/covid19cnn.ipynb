{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42330 files belonging to 2 classes.\n",
      "Using 33864 files for training.\n",
      "Found 42330 files belonging to 2 classes.\n",
      "Using 8466 files for validation.\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "COLOR_MODE = \"rgb\"\n",
    "CHANNELS = 1 if COLOR_MODE == \"grayscale\" else 3\n",
    "IMAGE_HEIGHT = 150\n",
    "IMAGE_WIDTH = 150\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "TRAINING_PATH = \"./dataset\"\n",
    "TESTING_PATH = \"./dataset/Test_Set/Test_Set\"\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    subset='training',\n",
    "    validation_split=0.2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_PATH,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation',\n",
    "    validation_split=0.2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(train_ds)\n",
    "# print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# def load_images_from_directory(directory):\n",
    "#     images = []\n",
    "#     masks = []\n",
    "#     labels = []\n",
    "#     class_names = os.listdir(directory)\n",
    "#     # class_names.sort()  # Ensure consistent order of classes\n",
    "\n",
    "#     for class_name in tqdm(class_names):\n",
    "#         # class_dir = os.path.join(directory, class_name)\n",
    "#         class_dir = f\"{directory}/{class_name}\"\n",
    "#         if os.path.isdir(class_dir):\n",
    "#             print(class_dir)\n",
    "#             for image_name in os.listdir(class_dir):\n",
    "#                 image_path = f\"{directory}/{class_name}/{image_name}\"\n",
    "#                 # mask_path = f\"{directory}/{class_name}/masks/{image_name}\"\n",
    "#                 # image = cv2.imread(image_path)\n",
    "#                 image = load_img(image_path, color_mode=COLOR_MODE, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), interpolation='bilinear')\n",
    "#                 # mask = cv2.imread(mask_path)\n",
    "#                 # mask = load_img(mask_path, color_mode=COLOR_MODE, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), interpolation='bilinear')\n",
    "#                 if image is not None:\n",
    "#                     images.append(img_to_array(image)/255.0)\n",
    "#                     # masks.append(img_to_array(mask)/255.0)\n",
    "#                     labels.append(class_name)\n",
    "    \n",
    "#     return images, labels, masks, class_names\n",
    "\n",
    "\n",
    "# dataset_directory = './dataset/images'\n",
    "# images, labels, masks, class_names = load_images_from_directory(dataset_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(image, mask):\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    return masked_image\n",
    "\n",
    "masked_images = []\n",
    "\n",
    "for i in range(len(images)):\n",
    "\t# mask = create_mask(image.shape)\n",
    "\tmasked_image = apply_mask(images[i], masks[i])\n",
    "\tmasked_images.append(masked_image)\n",
    "\t# Display or save the masked image\n",
    "\t# cv2.imshow('Masked Image', masked_image)\n",
    "\t# cv2.waitKey(0)  # Press any key to proceed\n",
    "\t# cv2.destroyAllWindows()\n",
    "\n",
    "\t# If you want to save the masked images, uncomment the following lines:\n",
    "\t# output_dir = 'path/to/save/masked_images'\n",
    "\t# os.makedirs(output_dir, exist_ok=True)\n",
    "\t# output_path = os.path.join(output_dir, f'masked_{labels[idx]}_{idx}.png')\n",
    "\t# cv2.imwrite(output_path, masked_image)\n",
    "# images = []\n",
    "masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import required module \n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "# read image \n",
    "# image = cv2.imread('gfg.png') \n",
    "  \n",
    "# call imshow() using plt object \n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "# plt.imshow(masks[0])\n",
    "# plt.show()\n",
    "# plt.imshow(masked_images[0])\n",
    "# plt.show()\n",
    "\n",
    "# display that image \n",
    "\n",
    "\n",
    "\n",
    "# cv2.imshow('Normal Image', images[0])\n",
    "# cv2.imshow('Mask', masks[0])\n",
    "# cv2.imshow('Masked Image', masked_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def randomize_dataset(images, labels):\n",
    "#     # Generate shuffled indices\n",
    "#     num_samples = len(images)\n",
    "#     shuffled_indices = np.random.permutation(num_samples)\n",
    "\n",
    "#     # Shuffle images and labels\n",
    "#     shuffled_images = images[shuffled_indices]\n",
    "#     shuffled_labels = labels[shuffled_indices]\n",
    "\n",
    "#     return shuffled_images, shuffled_labels\n",
    "\n",
    "# # Randomize the images in each dataset\n",
    "# train_ds_images, train_ds_labels = randomize_dataset(train_ds_images, train_ds_labels)\n",
    "# test_ds_images, test_ds_labels = randomize_dataset(test_ds_images, test_ds_labels)\n",
    "# validation_ds_images, validation_ds_labels = randomize_dataset(validation_ds_images, validation_ds_labels)\n",
    "\n",
    "# # Example of randomized dataset\n",
    "# print(\"Randomized train dataset:\")\n",
    "# print(train_ds_images.shape)\n",
    "# print(train_ds_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(masked_images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 75, 75, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 75, 75, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 75, 75, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 37, 37, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 37, 37, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 37, 37, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 18, 18, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 18, 18, 128)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,764\n",
      "Trainable params: 93,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# # Define the CNN model\n",
    "# model = Sequential([\n",
    "#     Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "\n",
    "    # preprocessing,\n",
    "\n",
    "    # keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "\n",
    "    keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    # keras.layers.Flatten(),\n",
    "    # keras.layers.Dropout(0.2),\n",
    "    # keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    # keras.layers.Dropout(0.2),\n",
    "    # keras.layers.Dense(256, activation=\"relu\"),\n",
    "    # keras.layers.Dropout(0.2),\n",
    "    # keras.layers.Dense(32, activation=\"relu\"),\n",
    "    # keras.layers.Dropout(0.2),\n",
    "\n",
    "    keras.layers.Dense(4, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(), \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary(\n",
    "    expand_nested=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Compute class weights\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(\u001b[43my_train\u001b[49m), y\u001b[38;5;241m=\u001b[39my_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert class weights to a dictionary format\u001b[39;00m\n\u001b[0;32m      8\u001b[0m class_weights_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(class_weights))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Convert class weights to a dictionary format\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# # Train the model\n",
    "EPOCHS = 100\n",
    "PATIENCE = 25\n",
    "earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    # train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    # validation_data=(x_test, y_test),\n",
    "    # validation_data=test_dataset,\n",
    "    callbacks=[earlystop_loss],\n",
    "    # class_weight=class_weights_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "265/265 [==============================] - 70s 247ms/step - loss: 0.6626 - sparse_categorical_accuracy: 0.9779 - val_loss: 5.3415e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 64s 241ms/step - loss: 4.6263e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.9776e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 65s 242ms/step - loss: 4.0147e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.0230e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 65s 243ms/step - loss: 2.8011e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.4643e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 65s 245ms/step - loss: 3.1065e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.4772e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 65s 242ms/step - loss: 2.2123e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9286e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "265/265 [==============================] - 65s 243ms/step - loss: 4.0140e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.0465e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "265/265 [==============================] - 65s 244ms/step - loss: 1.7462e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4628e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "265/265 [==============================] - 65s 245ms/step - loss: 1.5043e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4361e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "265/265 [==============================] - 66s 246ms/step - loss: 1.2609e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2335e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 1.5905e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5012e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 9.9718e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0683e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 1.0048e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0164e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 6.7137e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.9298e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 7.9464e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.1591e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "265/265 [==============================] - 63s 237ms/step - loss: 8.3267e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.5951e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "265/265 [==============================] - 63s 237ms/step - loss: 3.0835e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.3338e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 2.8885e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.6478e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "265/265 [==============================] - 63s 237ms/step - loss: 2.6801e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.1964e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 1.8718e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0510e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 1.4966e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.3764e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 2.1567e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0642e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 1.3389e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1785e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "265/265 [==============================] - 63s 237ms/step - loss: 2.4637e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.4763e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 9.7973e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.7915e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 7.7169e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.7418e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 6.8478e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.0688e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 6.2904e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.2712e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 5.9642e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.8449e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 4.7073e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8481e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 3.2998e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.0653e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 3.0411e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5832e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 9.6501e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.3937e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "265/265 [==============================] - 63s 236ms/step - loss: 2.5007e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2657e-05 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "265/265 [==============================] - 63s 235ms/step - loss: 1.7285e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.9165e-06 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "265/265 [==============================] - 63s 237ms/step - loss: 1.4617e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.3480e-06 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "106/265 [===========>..................] - ETA: 35s - loss: 7.9598e-08 - sparse_categorical_accuracy: 1.0000"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# # Train the model\n",
    "EPOCHS = 100\n",
    "PATIENCE = 25\n",
    "earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # validation_split=0.2,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[earlystop_loss],\n",
    "    # class_weight=class_weights_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_ds_images, test_ds_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Epochs run:\", len(history.history[\"loss\"]))\n",
    "\n",
    "acc = history.history[\"binary_accuracy\"]\n",
    "val_acc = history.history[\"val_binary_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Train and validation accuracy\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylim((0, 1))\n",
    "plt.plot(epochs, acc, label=\"Training accurarcy\")\n",
    "plt.plot(epochs, val_acc, label=\"Validation accurarcy\")\n",
    "plt.title(\"Training and Validation accurarcy\")\n",
    "plt.legend()\n",
    "\n",
    "# Train and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(epochs, loss, label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     TESTING_FOLER,\n",
    "#     image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "#     batch_size=32,\n",
    "#     color_mode=COLOR_MODE,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "# Get true labels and predicted probabilities\n",
    "true_labels = []\n",
    "\n",
    "predictions = model.predict(test_ds_images)\n",
    "# Convert probabilities to predicted labels\n",
    "predicted_labels = np.where(np.array(predictions) > 0.5, 1, 0)\n",
    "\n",
    "# Convert true labels and predicted labels to numpy arrays\n",
    "true_labels = np.array(test_ds_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Build the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(conf_matrix)\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
